# [Data Operations](mailto:data-operations-team@data.humancellatlas.org)


## Description

The Data Coordination Platform (DCP) Data Operations Team (DataOps) is a user-focused group aiming to connect the scientific community to the DCP through the effective tools and resources generated by the DCP which maximize the value and use of HCA data. This involves identifying the value added for the HCA consumers from various DCP components and the needs and expectations of the consumers, and connecting these communities through useful, fully vetted products and their distribution. These products include data, metadata, analysis tools, data access features. DataOps ensures that data flows through the DCP end-to-end in an efficient manner, is appropriately validated, and fulfills the needs and expectations of data consumers. DataOps maintains a DCP-wide vision through their internal focus on end-to-end data flow, as well as their community-focused goal of conveying the value of the entire DCP.

## Definitions

*HCA submission*: a set of raw data derived from the same experimental process, and the metadata describing that process

*Release* (verb): to make metadata and data openly accessible

*Data distribution*: the packaging of metadata and data from multiple HCA submissions in meaningful ways, along with relevant analysis pipelines, possible integrative analysis results, user vignettes, access features, and any announcements or documentation supporting it

*HCA Snapshot*: a type of data distribution that occurs on a regular cadence containing all HCA data currently available that meets predetermined criteria, such as quality standards

*Data lifecycle*: the sequence of tasks executed by the DCP that each HCA submission goes through beginning with first notification of a dataset (typically contact with the data generator or identification of data in a data archive or in scientific literature) all the way through data analysis by the DCP and continued through usability by data consumers

## Objectives

The objective of the Data Operations is to connect the scientific community to tools and resources from the DCP which maximize the value and use of HCA data such as data, metadata, analysis tools, and data access features. Data Operations aims to maintain a reliable and efficient data lifecycle and shepherds data through the data lifecycle.

## In-scope

- Data operations
  - Engage with all DCP components to identify deliverables, including data, pipelines, metadata, standards, and user features, which maximize the value and consistency of HCA data
  - Ensure the scientific community is connected with the identified deliverables from the DCP through data distributions or other means, as required by the community
  - Partner with all DCP components to ensure thorough DCP-wide end-to-end testing for features and components involved in the data lifecycle before being released and continued QA checks after being released to ensure data and data resource integrity and useability through component updates
  - Identify data lifecycle- and data distribution-associated engineering needs of all DCP components and participate with other components in crafting technical specifications
  - Establish and integrate into the DCP roadmap a reproducible plan that includes the specifications for and the timing of iterative data distribution to the community
  - Monitor the status of HCA submissions at each step in the DCP data lifecycle from initial project notification and shepherd all the way through release and potential inclusion on tertiary portals and/or in additional data distribution
  - Identify processes in the data lifecycle that require improved monitoring, streamlined troubleshooting, automation, etc. towards a more scalable and efficient data lifecycle with increased HCA submission bandwidth
  - Engage with Analysis Work Group to identify data quality control standards for each data distribution
  - Engage with all DCP components to ensure proper checks of data quality standards are put in place within the appropriate components throughout the data lifecycle
  - Engage with the [Compliance Working Group](https://github.com/HumanCellAtlas/dcp-community/blob/master/charters/Compliance-WG/charter.md) to ensure the our data release and distribution processes ensure responsible HCA data sharing to all types of consumers
  - Ensure released data that are used on tertiary portals can be easily cross-reference across portals
  - Determine best practices for when and how changes are made to DCP outputs after release

- Community engagement
  - Define data use policy and citation instructions for HCA data users
  - Define mechanism(s) for which the community is notified when DCP products are updated after being released
  - Partner with Content and Portal Development to ensure clear, findable, and current documentation describing the data lifecycle and data distribution
  - Ensure effective communication of HCA data updates and user features to the scientific community
  - Partner with UX to assess the added value of data distribution through communitiesâ€™ feedback, and better understand their expectations and needs
  - Onboard staff to DCP ZenDesk ticketing system
  - Take ownership of establishing process and interfacing with DCP components for assigning, responding, and integrating ZenDesk tickets
  - Compile feedback gained from consumers through discussions at scientific conferences and ZenDesk queries and comments. 
  - Report feedback to the DCP, at large, and the specific components pertaining to the feedback, in order to facilitate system improvement. The mechanism for reporting may be DataOps creating a feature ticket in the appropriate repo and/or presenting findings at a PM call and/or a feature proposal in the form of an RFC. DataOps will follow-up on outcomes, and whenever possible, keep the community reporter engaged throughout the follow-up process, ideally through direct interaction with the appropriate compoenent(s).


### Scientific "guardrails"

In order to meet the needs of the communities of scientific consumers, the DataOps team requires input from the Analysis Working Group as to where curated data distribution will add value for the researcher.

## Out-of-scope

The Data Operations is not responsible for determining what data or data types are made available through the DCP.

As many of the features and requirements for the data lifecycle and data distribution will be implemented through other components, the majority of technical aspects pertaining to the data lifecycle and data distribution will be implemented by the appropriate component, and not Data Operations.

## Milestones and Deliverables

2019 Q3
- Follow a dataset through the entire data lifecycle in order to understand each step and assess the current process.
- Organize incentivized HCA submission program in conjunction with the HCA General Meeting in Barcelona in order to encourage pre-publication HCA submissions. 
- Establish ZenDesk ticket triage procedure.

2019 Q4
- Write actionable RFCs aimed to improve the data lifecycle.
- Establish an MVP of features and improvements that are the prerequisite to a first Data Distribution

2020 Q1
- Determine a reproducible procedure for a periodic data snapshot including:
  - How to determine what data to include
  - How the data are organized
  - How to document and announce the distribution.
- Issue the first HCA Snapshot


## Roles

### Project Lead

[Jason Hilton](mailto:jahilton@stanford.edu)

### Product Owner

[Jennifer Zamanian](mailto:jlz@stanford.edu)

### Technical Lead

[J. Seth Strattan](mailto:jseth@stanford.edu)

## Communication
### Slack Channel
[HumanCellAtlas/data-ops](https://humancellatlas.slack.com/messages/data-ops)

### Slack Group
@dataops

### Mailing List
Team email: data-operations-team@data.humancellatlas.org
